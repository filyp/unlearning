model_id: meta-llama/Llama-3.2-1B
category: bio
split: dev_T
unlearning_loss_fn: neg_cross_entropy
masking_method: disruption_mask_avg  # null, disruption_mask_avg, disruption_mask_each
unlearning_method: normal  # normal, common_core
unlearning_rate: 5e-2
use_related_retain: false
# percentile: None
num_disruption_batches: 16
tokenizer:
    max_length: 128
    padding: true
    truncation: true
    return_tensors: pt
