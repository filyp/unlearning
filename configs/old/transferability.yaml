model_id: meta-llama/Llama-3.2-1B
category: bio
split: dev_T
split_disr: dev_V
# unlearning_loss_fn: correct_logit_minus_avg
unlearning_loss_fn: neg_cross_entropy

num_epochs: 10
unlearning_rate: 3e-4
# percentile: None
num_disruption_batches: 16
tokenizer:
    max_length: 128
    padding: true
    truncation: true
    return_tensors: pt
target_modules:
    - gate_proj
    - up_proj
    - down_proj
    - q_proj
    - k_proj
    - o_proj
    - v_proj
    # - .0.mlp.gate_proj.weight
    # - .1.mlp.gate_proj.weight
    # - .2.mlp.gate_proj.weight
    # - .3.mlp.gate_proj.weight
    # - .4.mlp.gate_proj.weight
    # - .5.mlp.gate_proj.weight