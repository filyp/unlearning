# general, unlikely to change
tokenizer:
    max_length: 128
    padding: true
    truncation: true
    return_tensors: pt
target_modules:
    - gate_proj
    - up_proj
    - down_proj
batch_size: 2  # 4 is the biggest we can go with 8b, on 40gb gpu

# model_id: meta-llama/Llama-3.2-3B
# model_id: meta-llama/Llama-3.2-1B
model_id: meta-llama/Llama-3.1-8B
dataset: wmdp_bio
eval_on_all_questions: false

max_num_epochs: 30

retraining_rate: 1e-6
retraining_epochs: 20

# retaining_rate: 1e-5
# retaining_loss_fn: cross_entropy
# retain_on_context: false
# retain_to_original: false
# use_wikitext_as_retain: true

# default experiment config
loss_fn_name: neg_cross_entropy
only_train_on_answer: false
algorithm: GA
loss_budget: 1.05
act_pca_num: 6
grad_pca_num: 6
cir_niter: 8
use_wikitext_as_retain: false

experiment_number: 0
experiment_list:

    - unlearning_rate: 3e-6
      retain_to_original: false

    - unlearning_rate: 3e-6
      model_id: meta-llama/Meta-Llama-3-8B
      dataset: wmdp_bio_deebs
      batch_size: 2  # 2 is needed if we're using deeb's 
        
    - unlearning_rate: 3e-6
      only_train_on_answer: true
      retaining_rate: 1e-5
      retaining_loss_fn: kl_loss
      retain_on_context: true
      retain_to_original: true
        
    - unlearning_rate: 3e-4
      algorithm: CIR
      loss_fn_name: correct_logit
      only_train_on_answer: true
      recalc_every_n_epochs: 5
      # model_id: meta-llama/Llama-3.2-1B
        
    - unlearning_rate: 3e-4
      algorithm: CIR
      loss_fn_name: correct_logit
      only_train_on_answer: true
      recalc_every_n_epochs: 5
      retaining_rate: 1e-3
      retaining_loss_fn: kl_loss
      retain_on_context: true
      retain_to_original: true
      # model_id: meta-llama/Llama-3.2-1B
        
    - unlearning_rate: 3e-4
      algorithm: CIR
      loss_fn_name: correct_logit
      only_train_on_answer: true
      recalc_every_n_epochs: 5
      retaining_rate: 1e-3
      retaining_loss_fn: kl_loss
      retain_on_context: false
      retain_to_original: true
        
    - unlearning_rate: 3e-4
      algorithm: CIR
      loss_fn_name: correct_logit
      only_train_on_answer: true
      recalc_every_n_epochs: 5
      retaining_rate: 3e-4
      retaining_loss_fn: kl_loss
      retain_on_context: false
      retain_to_original: true
        
    - unlearning_rate: 3e-4
      algorithm: CIR
      loss_fn_name: correct_logit
      only_train_on_answer: true
      recalc_every_n_epochs: 5
      retaining_rate: 1e-4
      retaining_loss_fn: kl_loss
      retain_on_context: false
      retain_to_original: false