# model_id: meta-llama/Llama-3.2-3B
model_id: meta-llama/Llama-3.2-1B
# model_id: meta-llama/Llama-3.1-8B
dataset: wmdp_bio
batch_size: 8
normalize: true
eval_on_all_questions: false

retraining_rate: 1e-6
retraining_epochs: 10
  
loss_budget: 1.05

max_num_epochs: 50
tokenizer:
    max_length: 128
    padding: true
    truncation: true
    return_tensors: pt
target_modules:
    - gate_proj
    - up_proj
    - down_proj

default_experiment_cfg:
    algorithm: GA
    loss_fn_name: neg_cross_entropy
    only_train_on_answer: false
    retain_on_context: false
    retain_to_original: false

experiment_number: 0
experiment_list:

    - unlearning_rate: 1e-6
      retaining_rate: 1e-4
      retaining_loss_fn: cross_entropy
      use_wikitext_as_retain: true

    - unlearning_rate: 1e-6
      retaining_rate: 3e-4
      retaining_loss_fn: cross_entropy
      use_wikitext_as_retain: true

    - unlearning_rate: 1e-6
      retaining_rate: 3e-5
      retaining_loss_fn: cross_entropy
      use_wikitext_as_retain: true
