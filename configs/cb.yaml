# general, unlikely to change
tokenizer:
    max_length: 128
    padding: true
    truncation: true
    return_tensors: pt
target_modules:
    - gate_proj
    - up_proj
    - down_proj
batch_size: 4  # with 8b on 40gb gpu, 2 or 4 is the max
retain_batch_size: 2
train_batch_size: 6  # can be higher because batches are usually shorter
num_eval_batches: 16  # will make it slower, but for final plots make it bigger

# model_id: meta-llama/Llama-3.2-3B
# model_id: meta-llama/Llama-3.2-1B
model_id: meta-llama/Llama-3.1-8B

# dataset: wmdp_cyber_pairs
# dataset: wmdp_cyber_pairs_only_ans
# dataset: wmdp_cyber_deebs
dataset: wmdp_cyber_simple
num_examples_per_question: 3
use_dev_split: false
# use_dev_split: true

max_num_epochs: 60

# retraining_rate: 2e-6  # even with 1e-6, wikitext_loss has this Nike shape
retraining_rate: 1e-6
retraining_epochs: 60
eval_on_all_questions: false

# default experiment config
algorithm: CIR
loss_budget: 1.01
cir_niter: 8
comment: null
mask_n_most_common_tokens: null
# mask_n_most_common_tokens: 50
# eval_on_all_questions: true  # best to just use it when not retraining
# rep_proj_num: 0 # deprecated
#
cut_off_tokens: 1
cb_floor: 0.0
cb_pow: 1.0
# loss_fn_name: neg_cross_entropy
# loss_fn_name: circuit_breaker
loss_fn_name: circuit_breaker
act_proj_num: 6
grad_proj_num: 6

experiment_number: 0
experiment_list:

    - unlearning_rate: 7e-4
      loss_fn_name: correct_logit
      act_proj_num: 6
      grad_proj_num: 1

    - unlearning_rate: 3e-2
      cb_layers: [8, 12]

    - unlearning_rate: 3e-2
      cb_layers: [6]
      cb_floor: 0.3

    - unlearning_rate: 2.66e-2
      cb_layers: [6]
      cb_floor: 0.0
      # cb_pow: 2.0
      cb_pow: 1.5

    - unlearning_rate: 3e-2
      cb_layers: [6, 7, 8, 9, 10]
      cb_floor: 0.0
      cb_pow: 1.0

    - unlearning_rate: 3e-2
      cb_layers: [6, 7, 8, 9, 10]
      cb_floor: 0.0
      cb_pow: 2.0

    - unlearning_rate: 3e-2
      cb_layers: [6, 7, 8, 9, 10]
      cb_floor: 0.0
      cb_pow: 1.5

    - unlearning_rate: 3e-2
      cb_layers: [6, 7, 8, 9, 10, 11, 12]
      cb_floor: 0.0
      cb_pow: 1.5























    # - unlearning_rate: 7e-4
    #   loss_fn_name: circuit_breaker
    #   cb_layers: [8, 12]
    #   act_proj_num: 0
    #   grad_proj_num: 0


    # - unlearning_rate: 3e-2
    #   loss_fn_name: circuit_breaker
    #   cb_layer_idx: 8
    #   act_proj_num: 6
    #   grad_proj_num: 6
    #   cb_floor: 0.3
    #   mask_n_most_common_tokens: 50


    # - unlearning_rate: 7e-4
    #   loss_fn_name: circuit_breaker
    #   cb_layer_idx: 8
    #   act_proj_num: 6
    #   grad_proj_num: 6
    #   rep_proj_num: 0
    #   num_examples_per_question: 7

    # - unlearning_rate: 7e-4
    #   loss_fn_name: circuit_breaker
    #   cb_layer_idx: 8
    #   act_proj_num: 6
    #   grad_proj_num: 6
    #   rep_proj_num: 4
    #   num_examples_per_question: 3

    # - unlearning_rate: 7e-4
    #   loss_fn_name: circuit_breaker
    #   cb_layer_idx: 8
    #   act_proj_num: 6
    #   grad_proj_num: 0
    #   rep_proj_num: 4
    #   num_examples_per_question: 3









    # - unlearning_rate: 1e-4
    #   act_proj_num: 6
    #   grad_proj_num: 1
    #   num_examples_per_question: 3
    #   ignore_bos: false

    # - unlearning_rate: 0.5e-3
    #   act_proj_num: 6
    #   grad_proj_num: 2
    #   dataset: wmdp_cyber_deebs
    #   train_batch_size: 2  # 4 is too much for Deeb's dataset
    #   loss_fn_name: correct_logit

    # - unlearning_rate: 1e-6
    #   algorithm: GA
    #   loss_fn_name: neg_cross_entropy
       
    # # sanity check
    # - unlearning_rate: 1e-6
    #   loss_fn_name: neg_cross_entropy
    #   act_proj_num: 0
    #   grad_proj_num: 0
    #   ignore_bos: false


# retraining_rate: 1e-6
# retraining_epochs: 30

# retaining_rate: 1e-5
# retaining_loss_fn: cross_entropy
# retain_on_context: false
# retain_to_original: false