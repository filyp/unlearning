model_id: meta-llama/Llama-3.2-1B
category: bio
split: dev_T
split_disr: dev_V
# unlearning_loss_fn: correct_logit_minus_avg
unlearning_loss_fn: neg_cross_entropy

retain_only_in_reverting_direction: true
unlearning_method: normal  # normal, common_core, only_answer_tokens
masking_method: null  # null, mask_out_answer_without_context

num_epochs: 40
unlearning_rate: 3e-4
# percentile: None
num_disruption_batches: 16
tokenizer:
    max_length: 128
    padding: true
    truncation: true
    return_tensors: pt
target_modules:
    - gate_proj
    # - up_proj
    # - down_proj
    # - q_proj
    # - k_proj
    # - o_proj
    # - v_proj
    # - .0.mlp.gate_proj.weight
    # - .1.mlp.gate_proj.weight
    # - .2.mlp.gate_proj.weight
    # - .3.mlp.gate_proj.weight
    # - .4.mlp.gate_proj.weight
    # - .5.mlp.gate_proj.weight