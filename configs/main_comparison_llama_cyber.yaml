# general, unlikely to change
tokenizer:
    max_length: 128
    padding: true
    truncation: true
    return_tensors: pt
train_batch_size: 32  # can be higher because batches are usually shorter
wikitext_batch_size: 16
retain_batch_size: 16
num_eval_batches: 16
target_modules:
  - gate_proj
  - up_proj
  - down_proj

# model_id: meta-llama/Llama-3.2-3B
# model_id: meta-llama/Llama-3.2-1B
model_id: meta-llama/Llama-3.1-8B

# dataset: wmdp_cyber_pairs
# dataset: wmdp_cyber_pairs_only_ans
# dataset: wmdp_cyber_deebs
dataset: wmdp_cyber_simple
num_examples_per_question: 3
use_dev_split: false

max_num_epochs: 200

retraining_rate: 1e-5  # even with 1e-6, wikitext_loss has this Nike shape
retraining_epochs: 100
eval_on_all_questions: false

# default experiment config
algorithm: CIR
loss_budget: 1.001
cir_niter: 16
comment: null
mask_n_most_common_tokens: null
# mask_n_most_common_tokens: 50
# eval_on_all_questions: true  # best to just use it when not retraining
#
cut_off_tokens: 1
loss_fn_name: ???
mlp_floor: 0.0

max_norm: ???
act_proj_num: 24
grad_proj_num: 36
layer_range: [6, 12]

# cb_type: dot
cb_type: cossim
cb_floor: 0.0
cb_pow: 1.0
# mlp_retaining_pow: 1.0

# this is used both for CIR and for CB
retaining_rate: 3e-4  # 1e-3 is fine too, but let's be safe and slow
retaining_loss_fns: [cb_retain]
cb_retaining_layers: [12]
cb_retaining_pow: 1.0

retain_on_dev: false
pca_every_n: 1

# let's search 3 OOMs, using 9 runs, and a constant low retaining rate
#
# for GA:
# retaining_rate: 1e-5  # 3e-5 is fine too, but let's be safe

experiment_list:
    
    # 0 too slow
    - loss_fn_name: mlp_confuse
      max_norm: 0.02

    # 1 ok
    - loss_fn_name: mlp_confuse
      max_norm: 0.05

    # 2 ok
    - loss_fn_name: mlp_confuse
      max_norm: 0.1

    # 3 ok - BEST
    - loss_fn_name: mlp_confuse
      max_norm: 0.2

    # 4 diverged
    - loss_fn_name: mlp_confuse
      max_norm: 0.5


    # 5 too slow
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.01
      max_norm: 0.001

    # 6 ok - but a bit too fast, let's try 0.0015
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.01
      max_norm: 0.002

    # 7 diverged
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.01
      max_norm: 0.005

    # 8 diverged
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.01
      max_norm: 0.01

    # 9 diverged
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.01
      max_norm: 0.02


    # 10 too slow
    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      loss_budget: 1.01
      max_norm: 0.00002

    # 11 too slow
    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      loss_budget: 1.01
      max_norm: 0.00005

    # 12 ok - best
    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      loss_budget: 1.01
      max_norm: 0.0001

    # 13 ok
    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      loss_budget: 1.01
      max_norm: 0.0002

    # 14 diverged
    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      loss_budget: 1.01
      max_norm: 0.0005



    # ####### 15-24 are all hopeless - the loss_budget is just too small
    - loss_fn_name: circuit_breaker
      algorithm: GA
      max_norm: 0.001

    - loss_fn_name: circuit_breaker
      algorithm: GA
      max_norm: 0.002

    - loss_fn_name: circuit_breaker
      algorithm: GA
      max_norm: 0.005

    - loss_fn_name: circuit_breaker
      algorithm: GA
      max_norm: 0.01

    - loss_fn_name: circuit_breaker
      algorithm: GA
      max_norm: 0.02



    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      max_norm: 0.00002

    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      max_norm: 0.00005

    # 22 - 12 repeated with 10x smaller loss_budget
    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      max_norm: 0.0001


    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      max_norm: 0.0002

    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      max_norm: 0.0005



    # bonus: no retain - kinda ok: 41% acc instead of 37% with retain
    - loss_fn_name: mlp_confuse
      max_norm: 0.1
      retaining_rate: 0

    # hopeless
    - loss_fn_name: circuit_breaker
      algorithm: GA
      max_norm: 0.002
      retaining_rate: 0

    # hopeless (btw, this is mistakenly duplicated, so ignore it)
    - loss_fn_name: circuit_breaker
      algorithm: GA
      max_norm: 0.002
      retaining_rate: 0



    # 28
    - loss_fn_name: mlp_confuse
      max_norm: 0.2
      loss_budget: 1.002

    - loss_fn_name: mlp_confuse
      max_norm: 0.2
      loss_budget: 1.005

    # 30 best of mlp_confuse repeated with 10x loss_budget
    - loss_fn_name: mlp_confuse
      max_norm: 0.2
      loss_budget: 1.01


    # 31 baseline
    - loss_fn_name: mlp_confuse
      max_num_epochs: 1
      max_norm: 0.0


    # 32 - very weird: goes well for a while and then reverses! as if the retaining won over the struggle
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.01
      max_norm: 0.0015

    # 33 - 32 with 10x smaller loss_budget
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.001
      max_norm: 0.0015

    # 34 - dummy
    - comment: dummy




    # runs with 3% disruption ########################################

    # 35
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.03
      max_norm: 0.001

    # 36
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.03
      max_norm: 0.002

    # 37
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.03
      max_norm: 0.005

    # 38
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.03
      max_norm: 0.01

    # 39
    - loss_fn_name: circuit_breaker
      algorithm: GA
      loss_budget: 1.03
      max_norm: 0.02


    # 40
    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      loss_budget: 1.03
      max_norm: 0.00002

    # 41
    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      loss_budget: 1.03
      max_norm: 0.00005

    # 42 - previous best
    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      loss_budget: 1.03
      max_norm: 0.0001

    # 43
    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      loss_budget: 1.03
      max_norm: 0.0002

    # 44
    - loss_fn_name: neg_cross_entropy
      algorithm: GA
      retaining_rate: 1e-5
      retaining_loss_fns: [cross_entropy]
      loss_budget: 1.03
      max_norm: 0.0005


    # 45 - 30 but with 3% not 1% disruption
    - loss_fn_name: mlp_confuse
      max_norm: 0.2
      loss_budget: 1.03


    # 46 - forked 3, but without any eval (it was commented out)
    - loss_fn_name: mlp_confuse
      max_norm: 0.2
      max_num_epochs: 9

    # 47 - this one is identical to 3
    - loss_fn_name: mlp_confuse
      max_norm: 0.2
      max_num_epochs: 10

    - loss_fn_name: mlp_confuse
      max_norm: 0.2
      max_num_epochs: 8

    # 49 - forked 3, but without any eval (it was commented out) and also rarer PCA
    # actually it breaks wikitext too much!
    - loss_fn_name: mlp_confuse
      max_norm: 0.2
      max_num_epochs: 10
      pca_every_n: 20

    # 50 - fork of 3
    - loss_fn_name: mlp_confuse
      max_norm: 0.2
      retain_on_dev: true

    # 51 - fork of 50
    - loss_fn_name: mlp_confuse
      max_norm: 0.2
      retain_on_dev: true
      max_num_epochs: 10

    # 52 - 3 duplicate
    - loss_fn_name: mlp_confuse
      max_norm: 0.2
      retraining_epochs: 15




