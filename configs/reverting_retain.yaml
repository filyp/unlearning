model_id: meta-llama/Llama-3.2-1B
category: bio
split: dev_T
unlearning_loss_fn: neg_cross_entropy

precompute_unlearning_grads: true
retain_only_in_reverting_direction: true
unlearning_method: normal  # normal, common_core, only_answer_tokens
masking_method: null  # null, mask_out_answer_without_context

num_epochs: 20
unlearning_steps: 300
unlearning_rate: 20e-2
retaining_rate: 2e-2
# percentile: None
num_disruption_batches: 8
tokenizer:
    max_length: 128
    padding: true
    truncation: true
    return_tensors: pt
target_modules:
    - gate_proj
    - up_proj
    # - down_proj
    - q_proj
    - k_proj
    - o_proj
    # - v_proj