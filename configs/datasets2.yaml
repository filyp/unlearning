# general, unlikely to change
tokenizer:
    max_length: 128
    padding: true
    truncation: true
    return_tensors: pt
target_modules:
    - gate_proj
    - up_proj
    - down_proj
batch_size: 2  # 4 is the biggest we can go with 8b, on 40gb gpu

# model_id: meta-llama/Llama-3.2-3B
model_id: meta-llama/Llama-3.2-1B
# model_id: meta-llama/Llama-3.1-8B  # todo use 8b

dataset: wmdp_cyber_pairs
# dataset: wmdp_cyber_deebs
# dataset: wmdp_cyber_simple

max_num_epochs: 30

# default experiment config
algorithm: CIR
# loss_fn_name: correct_logit
loss_fn_name: neg_cross_entropy
only_train_on_answer: false
recalc_every_n_epochs: 5
loss_budget: 1.01
cir_niter: 8
# max_norm: 0.1
extra: null
ignore_bos: true

experiment_number: 0
experiment_list:

    - unlearning_rate: 1e-4
      act_proj_num: 6
      grad_proj_num: 0
      control_on_training: true

    - unlearning_rate: 1e-4
      act_proj_num: 6
      grad_proj_num: 0
      control_on_training: false

    # - unlearning_rate: 1e-6
    #   algorithm: GA
    #   loss_fn_name: neg_cross_entropy
       
    # # sanity check
    # - unlearning_rate: 1e-6
    #   loss_fn_name: neg_cross_entropy
    #   act_proj_num: 0
    #   grad_proj_num: 0
    #   ignore_bos: false


# retraining_rate: 1e-6
# retraining_epochs: 30

# retaining_rate: 1e-5
# retaining_loss_fn: cross_entropy
# retain_on_context: false
# retain_to_original: false