# %%
import random

from transformers import AutoTokenizer

from datasets import load_dataset

base_url = "https://raw.githubusercontent.com/aghyad-deeb/unlearning_evaluation/refs/heads/main/data"
# corpora generated by GPT-4o are corpus_*.jsonl
data_paths = dict(
    wmdp_deduped_unlearning=[
        "wmdp-deduped/corpus_split_0.jsonl",
        "wmdp-deduped/corpus_split_1.jsonl",
        "wmdp-deduped/corpus_split_2.jsonl",
        "wmdp-deduped/corpus_split_3.jsonl",
        "wmdp-deduped/corpus_split_4.jsonl",
    ],
    wmdp_deduped_relearning=[
        "wmdp-deduped/corpus_split_0.jsonl",
        "wmdp-deduped/corpus_split_1.jsonl",
        "wmdp-deduped/corpus_split_2.jsonl",
        "wmdp-deduped/corpus_split_3.jsonl",
    ],
    wmdp_deduped_mcq_eval=[
        "wmdp-deduped/split_4.jsonl",
    ],

    wmdp_deduped_mcq_full=[
        "wmdp-deduped/split_0.jsonl",
        "wmdp-deduped/split_1.jsonl",
        "wmdp-deduped/split_2.jsonl",
        "wmdp-deduped/split_3.jsonl",
        "wmdp-deduped/split_4.jsonl",
    ],

    wmdp_deduped_wrong_answers=[
        "wmdp-deduped/whp_corpus_split_0.jsonl",
        "wmdp-deduped/whp_corpus_split_1.jsonl",
        "wmdp-deduped/whp_corpus_split_2.jsonl",
        "wmdp-deduped/whp_corpus_split_3.jsonl",
        "wmdp-deduped/whp_corpus_split_4.jsonl",
    ],
    # years_unlearning=[
    #     "dates-years-trimmed/corpus_split_0.jsonl",
    #     "dates-years-trimmed/corpus_split_1.jsonl",
    #     "dates-years-trimmed/corpus_split_2.jsonl",
    #     "dates-years-trimmed/corpus_split_3.jsonl",
    #     "dates-years-trimmed/corpus_split_4.jsonl",
    # ],
    # years_relearning=[
    #     "dates-years-trimmed/corpus_split_0.jsonl",
    #     "dates-years-trimmed/corpus_split_1.jsonl",
    #     "dates-years-trimmed/corpus_split_2.jsonl",
    #     "dates-years-trimmed/corpus_split_3.jsonl",
    # ],
    # years_mcq_eval=[
    #     "dates-years-trimmed/split_4.jsonl",
    # ],
    # mmlu_forget = [
    #     "mmlu_cats_random_trimmed/corpus_mmlu_STEM.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_business.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_chemistry.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_culture.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_geography.jsonl",
    # ],
    mmlu_retain=[
        "mmlu_cats_random_trimmed/corpus_mmlu_health.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_history.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_law.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_philosophy.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_social sciences.jsonl",
    ],
)


# bio=1065 cyber=1179 other=111, where other are due to small updates to official WMDP
def load_low_mi_set(paths):
    return load_dataset(
        "json", data_files=[f"{base_url}/{path}" for path in paths], split="train"
    )


# %%

def filter_by_question(corpus, category="bio", portion=1):
    # load the origianl wmdp

    # optionally filter by category and cut out some portion
    # we must reference the original wmdp, because deduped version has no category info
    if category == "bio":
        _wmdp_bio = load_dataset("cais/wmdp", "wmdp-bio")["test"].shuffle(seed=42)
        bio_questions = [ex["question"] for ex in _wmdp_bio]
        target_size = int(len(bio_questions) * portion)
        questions = bio_questions[:target_size]
    elif category == "cyber":
        _wmdp_cyber = load_dataset("cais/wmdp", "wmdp-cyber")["test"].shuffle(seed=42)
        cyber_questions = [ex["question"] for ex in _wmdp_cyber]
        target_size = int(len(cyber_questions) * portion)
        questions = cyber_questions[:target_size]
    else:
        raise ValueError(f"Invalid category: {category}, choose from bio or cyber")

    if "original_question" in corpus.features:
        return corpus.filter(lambda ex: ex["original_question"] in questions)
    else:
        return corpus.filter(lambda ex: ex["question"] in questions)


def load_batches(corpus, model_id, batch_size=4, max_length=128):
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token

    corpus = corpus.shuffle(seed=42)
    corpus = corpus.batch(batch_size)
    batches = [
        tokenizer(
            x["text"],
            max_length=max_length,
            padding=True,
            # padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        for x in corpus
    ]
    return batches


def load_retain_corpus(dataset_name):
    match dataset_name:
        case "fineweb_edu":
            corpus = load_dataset(
                "HuggingFaceFW/fineweb-edu",
                name="sample-10BT",
                split="train",
                # just the smallest parquet file will be enough
                data_files=["sample/10BT/013_00000.parquet"],
            )
            return corpus.select(range(2_500))
        case _:
            raise ValueError(f"Invalid dataset name: {dataset_name}")



# def load_filtered_mmlu_dataset():
#     # %% prepare filtered mmlu dataset
#     mmlu_dataset = load_dataset("cais/mmlu", "all", split="validation")

#     # filter out all the subcategories of biology and health
#     # keep even the ones like anatomy, clinical_knowledge and professional_medicine,
#     # because they contain some questions about molecular biology
#     categories_to_reject = {
#         "college_biology",
#         "college_medicine",
#         "high_school_biology",
#         "human_aging",
#         "medical_genetics",
#         "nutrition",
#         "professional_medicine",
#         "virology",
#         "anatomy",
#         "clinical_knowledge",
#     }
#     # filter out the ones in the categories_to_reject
#     return [ex for ex in mmlu_dataset if ex["subject"] not in categories_to_reject]
